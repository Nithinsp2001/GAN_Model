{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYDAfbufHQHKTN9o1yQCYo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nithinsp2001/GAN_Model/blob/main/6_Bias_%26_Fairness_Tools_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf7S5Ev1GHtc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Bias Metric: False Negative Rate (FNR) Parity**\n",
        "\n",
        "#### ✅ What it Measures:\n",
        "\n",
        "**False Negative Rate Parity** compares the **false negative rate** (FNR) across different groups (e.g., race, gender).\n",
        "\n",
        "$$\n",
        "\\text{FNR} = \\frac{\\text{False Negatives}}{\\text{False Negatives} + \\text{True Positives}}\n",
        "$$\n",
        "\n",
        "It checks whether the model **misses positive cases** (e.g., failing to identify someone who should have been approved, selected, or helped) **equally across groups**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###  Why It's Important:\n",
        "\n",
        "* Ensures fairness in **access to positive outcomes**.\n",
        "* A high FNR for a group means they are unfairly **denied opportunities** (e.g., jobs, loans, parole).\n",
        "* For example, in healthcare or hiring:\n",
        "\n",
        "  * If women have a higher FNR than men, qualified women might be underrepresented.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### How a Model Might Fail This Metric:\n",
        "\n",
        "A model trained on biased data may learn patterns that **underestimate the positive class** for certain groups.\n",
        "\n",
        "**Example:**\n",
        "In a loan approval model:\n",
        "\n",
        "* The model may falsely reject many qualified applicants from a minority group (high FNR).\n",
        "* This could result from historical data reflecting past discriminatory decisions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### (Optional) Using the Aequitas Demo Tool:\n",
        "\n",
        "Aequitas provides a **Jupyter notebook** or web demo for evaluating fairness metrics. If you apply it to the sample data:\n",
        "\n",
        "* Select **race**, **gender**, or other attributes as the group variable.\n",
        "* Aequitas will calculate FNR and compare it across groups.\n",
        "* If any group’s FNR deviates significantly from the reference group, it flags a **bias risk**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###  Conclusion:\n",
        "\n",
        "**False Negative Rate Parity** is crucial for evaluating whether a model **equitably identifies true positives** across demographic groups. Ensuring parity prevents systematic exclusion and promotes fairness in high-stakes decisions.\n",
        "\n",
        "Would you like help running Aequitas on a sample dataset in Python or uploading your own data?\n"
      ],
      "metadata": {
        "id": "5MPCz4neGIJ_"
      }
    }
  ]
}