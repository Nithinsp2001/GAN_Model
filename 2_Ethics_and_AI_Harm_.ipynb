{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBpxrTPMgG2GqTJY/dBQ+4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nithinsp2001/GAN_Model/blob/main/2_Ethics_and_AI_Harm_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apd9plXtDog1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs choose **Allocational Harm** ‚Äî a type of AI harm where an automated system unfairly affects access to opportunities, resources, or services.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### üìå **Example of Allocational Harm:**\n",
        "\n",
        "**Application:** AI-powered hiring systems used by companies to screen job applicants.\n",
        "\n",
        "**Real-world Scenario:**\n",
        "A company uses an AI tool trained on historical hiring data to evaluate resumes and rank applicants. The data reflects past biases ‚Äî for example, men were historically hired more often for tech roles. As a result, the model tends to rank male candidates higher than equally qualified female or minority candidates.\n",
        "\n",
        "üìâ **Harm Caused:**\n",
        "Qualified individuals may be unfairly filtered out of the job pipeline based on biased patterns in training data ‚Äî leading to systemic exclusion from employment opportunities.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### üõ°Ô∏è **Two Harm Mitigation Strategies:**\n",
        "\n",
        "#### 1. **Bias Auditing and Fairness Testing**\n",
        "\n",
        "* Regularly audit the model‚Äôs decisions across different demographic groups.\n",
        "* Use fairness metrics (like demographic parity or equal opportunity) to detect and correct disparities in outcomes.\n",
        "\n",
        "\n",
        "#### 2. **Human Oversight and Hybrid Decision-Making**\n",
        "\n",
        "* Ensure that AI is used as a support tool, not the sole decision-maker.\n",
        "* Allow human recruiters to review and override automated rejections, especially for underrepresented groups.\n",
        "\n",
        "---\n",
        "\n",
        "These strategies help promote fairness and accountability in AI applications, reducing the risk of systematic exclusion or discrimination.\n"
      ],
      "metadata": {
        "id": "kvDrgeM_Do4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5bgkI0rCEfrE"
      }
    }
  ]
}